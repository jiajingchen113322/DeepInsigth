{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-baa51z12\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from clip==1.0) (4.42.1)\n",
      "Requirement already satisfied: torch in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from clip==1.0) (1.7.0+cu110)\n",
      "Requirement already satisfied: torchvision in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from clip==1.0) (0.8.1+cu110)\n",
      "Collecting ftfy\n",
      "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "\u001b[K     |████████████████████████████████| 53 kB 2.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting regex\n",
      "  Downloading regex-2022.9.11-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (752 kB)\n",
      "\u001b[K     |████████████████████████████████| 752 kB 27.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: future in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from torch->clip==1.0) (0.18.2)\n",
      "Requirement already satisfied: typing-extensions in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from torch->clip==1.0) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from torch->clip==1.0) (1.19.4)\n",
      "Collecting dataclasses\n",
      "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from torchvision->clip==1.0) (7.0.0)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from torch->clip==1.0) (1.19.4)\n",
      "Requirement already satisfied: torch in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from clip==1.0) (1.7.0+cu110)\n",
      "Collecting wcwidth>=0.2.5\n",
      "  Downloading wcwidth-0.2.5-py2.py3-none-any.whl (30 kB)\n",
      "Building wheels for collected packages: clip\n",
      "  Building wheel for clip (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369396 sha256=6a86809c8089d846593109186093f3eed2650b95ad23999a74249ea7621ba455\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-nfc3b4xn/wheels/fd/b9/c3/5b4470e35ed76e174bff77c92f91da82098d5e35fd5bc8cdac\n",
      "Successfully built clip\n",
      "Installing collected packages: dataclasses, wcwidth, regex, ftfy, clip\n",
      "  Attempting uninstall: wcwidth\n",
      "    Found existing installation: wcwidth 0.1.8\n",
      "    Uninstalling wcwidth-0.1.8:\n",
      "      Successfully uninstalled wcwidth-0.1.8\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pytest-astropy 0.8.0 requires pytest-cov>=2.0, which is not installed.\n",
      "pytest-astropy 0.8.0 requires pytest-filter-subpackage>=0.1, which is not installed.\n",
      "fastai 1.0.61 requires nvidia-ml-py3, which is not installed.\u001b[0m\n",
      "Successfully installed clip-1.0 dataclasses-0.6 ftfy-6.1.1 regex-2022.9.11 wcwidth-0.2.5\n",
      "\u001b[33mWARNING: You are using pip version 20.3; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/home/ubuntu/anaconda3/envs/pytorch_latest_p37/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/clip/clip.py:24: UserWarning: PyTorch version 1.7.1 or higher is recommended\n",
      "  warnings.warn(\"PyTorch version 1.7.1 or higher is recommended\")\n",
      "100%|██████████| 160/160 [07:51<00:00,  2.95s/it]\n",
      "100%|██████████| 40/40 [01:58<00:00,  2.96s/it]\n",
      "100%|██████████| 25/25 [01:10<00:00,  2.83s/it]\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.96230599 0.99775281 0.96240602 0.98534799 0.98800959 0.99065421\n",
      " 0.99557522 0.94420601 0.98624754 0.98989899]\n",
      "Accuracy = 98.020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.5min finished\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import clip\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Load the model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load('ViT-L/14', device)\n",
    "\n",
    "# Load the dataset\n",
    "data_dir = 'video/imgs/train/'\n",
    "image_datasets = datasets.ImageFolder(data_dir, transform=preprocess)\n",
    "\n",
    "train, val, test = torch.utils.data.random_split(image_datasets, [16000, 4000, 2424])\n",
    "class_names = image_datasets.classes\n",
    "# data_transforms ={ 'train': preprocess, 'test': preprocess}\n",
    "\n",
    "# data_dir = 'video/imgs/'\n",
    "# image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "#                                           data_transforms[x])\n",
    "#                   for x in ['train', 'test']}\n",
    "# dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "#                                              shuffle=True, num_workers=4)\n",
    "#               for x in ['train', 'test']}\n",
    "# dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n",
    "# class_names = image_datasets['train'].classes\n",
    "\n",
    "\n",
    "\n",
    "def get_features(dataset):\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(DataLoader(dataset, batch_size=100)):\n",
    "            features = model.encode_image(images.to(device)) #feature shape([100,768])\n",
    "\n",
    "            all_features.append(features)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    return torch.cat(all_features).cpu().numpy(), torch.cat(all_labels).cpu().numpy()\n",
    "\n",
    "# Calculate the image features\n",
    "train_features, train_labels = get_features(train)\n",
    "val_features, val_labels = get_features(val)\n",
    "test_features, test_labels = get_features(test)\n",
    "\n",
    "# Perform logistic regression\n",
    "classifier = LogisticRegression(random_state=0, C=0.616, max_iter=3000, verbose=1)\n",
    "classifier.fit(train_features, train_labels)\n",
    "\n",
    "# Evaluate using the logistic regression classifier\n",
    "predictions = classifier.predict(test_features)\n",
    "print(f1_score(test_labels, predictions, average=None))\n",
    "\n",
    "accuracy = np.mean((test_labels == predictions).astype(np.float)) * 100.\n",
    "print(f\"Accuracy = {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparameter tuning grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-4c191e7aa74d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Get a batch of training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# inputs, classes = next(iter(dataloaders['train']))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    figure(figsize=(18, 16))\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "# Get a batch of training data\n",
    "# inputs, classes = next(iter(dataloaders['train']))\n",
    "inputs, classes = next(iter(tqdm(DataLoader(dataset, batch_size=5))))\n",
    "print(classes.shape)\n",
    "print(inputs.shape)\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "imshow(out, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "logModel = LogisticRegression()\n",
    "param_grid = [    \n",
    "    {'penalty' : ['l1','l2'],\n",
    "    'C' : np.logspace(-6, 6, 50),\n",
    "    'solver' : ['lbfgs','newton-cg'],\n",
    "    'max_iter' : [1000,2500,3000,4500]\n",
    "    }\n",
    "]\n",
    "clf = GridSearchCV(logModel, param_grid = param_grid, verbose=True, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 800 candidates, totalling 4000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   22.6s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   59.8s\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1792 tasks      | elapsed: 17.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2442 tasks      | elapsed: 52.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3192 tasks      | elapsed: 101.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4000 out of 4000 | elapsed: 119.3min finished\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "best_clf = clf.fit(val_features,val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=212.09508879201925, class_weight=None, dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=3000, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy - : 0.847\n"
     ]
    }
   ],
   "source": [
    "print (f'Accuracy - : {best_clf.score(test_features,test_labels):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                               precision    recall  f1-score   support\n",
      "\n",
      "      driver is adjusting his or her hair while driving a car       0.85      0.80      0.82        44\n",
      "   driver is drinking water from a bottle while driving a car       0.74      0.78      0.76        37\n",
      "                         driver is eating while driving a car       0.74      0.74      0.74        38\n",
      "   driver is picking something from floor while driving a car       0.93      0.90      0.92        30\n",
      "driver is reaching behind to the backseat while driving a car       0.95      0.98      0.97        43\n",
      "driver is singing a song with music and smiling while driving       0.73      0.85      0.79        39\n",
      "   driver is talking to the phone on hand while driving a car       0.96      0.94      0.95        51\n",
      "                        driver is yawning while driving a car       0.86      0.75      0.80        32\n",
      "\n",
      "                                                     accuracy                           0.85       314\n",
      "                                                    macro avg       0.85      0.84      0.84       314\n",
      "                                                 weighted avg       0.85      0.85      0.85       314\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_true = test_labels\n",
    "y_pred = predictions\n",
    "\n",
    "# average_precision_score(y_true, y_pred) #mAP\n",
    "# f1_score(y_true, y_pred, average=None) #f1\n",
    "# precision_score(y_true, y_pred, average=None) #precision\n",
    "# recall_score(y_true, y_pred, average=None) #recall\n",
    "\n",
    "# target_names = ['class 0', 'class 1', 'class 2', 'class 3','class 4','class 5','class 6','class 7']\n",
    "target_names = class_names\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ViT-B/16 backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 335M/335M [00:01<00:00, 184MiB/s]\n",
      "  0%|          | 0/90 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['driver is adjusting his or her hair while driving a car', 'driver is drinking water from a bottle while driving a car', 'driver is eating while driving a car', 'driver is picking something from floor while driving a car', 'driver is reaching behind to the backseat while driving a car', 'driver is singing a song with music and smiling while driving', 'driver is talking to the phone on hand while driving a car', 'driver is yawning while driving a car']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [01:21<00:00,  1.11it/s]\n",
      "100%|██████████| 20/20 [00:18<00:00,  1.09it/s]\n",
      "100%|██████████| 11/11 [00:09<00:00,  1.13it/s]\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74404762 0.68679245 0.57142857 0.87136929 0.85714286 0.6970684\n",
      " 0.87412587 0.55238095]\n",
      "Accuracy = 74.497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.1min finished\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import clip\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Load the model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load('ViT-B/16', device)\n",
    "\n",
    "# Load the dataset\n",
    "data_dir = 'new_data/'\n",
    "image_datasets = datasets.ImageFolder(data_dir, transform=preprocess)\n",
    "\n",
    "train, val, test = torch.utils.data.random_split(image_datasets, [9000, 2000, 1043])\n",
    "class_names = image_datasets.classes\n",
    "\n",
    "print(class_names)\n",
    "\n",
    "def get_features(dataset):\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(DataLoader(dataset, batch_size=100)):\n",
    "            features = model.encode_image(images.to(device)) #feature shape([100,768])\n",
    "\n",
    "            all_features.append(features)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    return torch.cat(all_features).cpu().numpy(), torch.cat(all_labels).cpu().numpy()\n",
    "\n",
    "# Calculate the image features\n",
    "train_features, train_labels = get_features(train)\n",
    "val_features, val_labels = get_features(val)\n",
    "test_features, test_labels = get_features(test)\n",
    "\n",
    "# Perform logistic regression\n",
    "classifier = LogisticRegression(random_state=0, C=0.42919342601287785, max_iter=2500, verbose=1)\n",
    "classifier.fit(train_features, train_labels)\n",
    "\n",
    "# Evaluate using the logistic regression classifier\n",
    "predictions = classifier.predict(test_features)\n",
    "print(f1_score(test_labels, predictions, average=None))\n",
    "\n",
    "accuracy = np.mean((test_labels == predictions).astype(np.float)) * 100.\n",
    "print(f\"Accuracy = {accuracy:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ViT-B/32 backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 338M/338M [00:01<00:00, 233MiB/s]\n",
      "100%|██████████| 80/80 [01:23<00:00,  1.05s/it]\n",
      "100%|██████████| 20/20 [00:20<00:00,  1.04s/it]\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.18it/s]\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.66666667 0.6984127  0.73239437 0.88571429 0.91891892 0.7047619\n",
      " 0.75862069 0.7012987 ]\n",
      "Accuracy = 75.478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.4min finished\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import clip\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Load the model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load('ViT-B/32', device)\n",
    "\n",
    "# Load the dataset\n",
    "data_dir = 'new_data/'\n",
    "image_datasets = datasets.ImageFolder(data_dir, transform=preprocess)\n",
    "\n",
    "train, val, test = torch.utils.data.random_split(image_datasets, [8000, 2000, 314])\n",
    "class_names = image_datasets.classes\n",
    "\n",
    "def get_features(dataset):\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(DataLoader(dataset, batch_size=100)):\n",
    "            features = model.encode_image(images.to(device)) #feature shape([100,768])\n",
    "\n",
    "            all_features.append(features)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    return torch.cat(all_features).cpu().numpy(), torch.cat(all_labels).cpu().numpy()\n",
    "\n",
    "# Calculate the image features\n",
    "train_features, train_labels = get_features(train)\n",
    "val_features, val_labels = get_features(val)\n",
    "test_features, test_labels = get_features(test)\n",
    "\n",
    "# Perform logistic regression\n",
    "classifier = LogisticRegression(random_state=0, C=0.42919342601287785, max_iter=2500, verbose=1)\n",
    "classifier.fit(train_features, train_labels)\n",
    "\n",
    "# Evaluate using the logistic regression classifier\n",
    "predictions = classifier.predict(test_features)\n",
    "print(f1_score(test_labels, predictions, average=None))\n",
    "\n",
    "accuracy = np.mean((test_labels == predictions).astype(np.float)) * 100.\n",
    "print(f\"Accuracy = {accuracy:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RN101 backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 278M/278M [00:03<00:00, 91.8MiB/s]\n",
      "100%|██████████| 80/80 [01:01<00:00,  1.31it/s]\n",
      "100%|██████████| 20/20 [00:15<00:00,  1.31it/s]\n",
      "100%|██████████| 4/4 [00:02<00:00,  1.47it/s]\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.57777778 0.65714286 0.41509434 0.80519481 0.79452055 0.52941176\n",
      " 0.64285714 0.50632911]\n",
      "Accuracy = 61.783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   12.3s finished\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import clip\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Load the model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load('RN101', device)\n",
    "\n",
    "# Load the dataset\n",
    "data_dir = 'new_data/'\n",
    "image_datasets = datasets.ImageFolder(data_dir, transform=preprocess)\n",
    "\n",
    "train, val, test = torch.utils.data.random_split(image_datasets, [8000, 2000, 314])\n",
    "class_names = image_datasets.classes\n",
    "\n",
    "def get_features(dataset):\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(DataLoader(dataset, batch_size=100)):\n",
    "            features = model.encode_image(images.to(device)) #feature shape([100,768])\n",
    "\n",
    "            all_features.append(features)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    return torch.cat(all_features).cpu().numpy(), torch.cat(all_labels).cpu().numpy()\n",
    "\n",
    "# Calculate the image features\n",
    "train_features, train_labels = get_features(train)\n",
    "val_features, val_labels = get_features(val)\n",
    "test_features, test_labels = get_features(test)\n",
    "\n",
    "# Perform logistic regression\n",
    "classifier = LogisticRegression(random_state=0, C=0.42919342601287785, max_iter=2500, verbose=1)\n",
    "classifier.fit(train_features, train_labels)\n",
    "\n",
    "# Evaluate using the logistic regression classifier\n",
    "predictions = classifier.predict(test_features)\n",
    "print(f1_score(test_labels, predictions, average=None))\n",
    "\n",
    "accuracy = np.mean((test_labels == predictions).astype(np.float)) * 100.\n",
    "print(f\"Accuracy = {accuracy:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['driver adjusting hair and makeup while driving a car', 'driver drinking water from a bottle while driving a car', 'driver normally driving a car', 'driver operating the radio while driving a car', 'driver reaching behind while driving a car', 'driver talking on the phone in left hand while driving a car', 'driver talking on the phone in right hand while driving a car', 'driver talking to passenger while driving a car', 'driver texting on the phone in left hand while driving a car', 'driver texting on the phone in right hand while driving a car']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import clip\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Load the model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load('ViT-L/14', device)\n",
    "\n",
    "# Load the dataset\n",
    "data_dir = 'video/imgs/train/'\n",
    "image_datasets = datasets.ImageFolder(data_dir, transform=preprocess)\n",
    "\n",
    "train, val, test = torch.utils.data.random_split(image_datasets, [16000, 4000, 2424])\n",
    "class_names = image_datasets.classes\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataset.Subset'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "b29f9eb3544a00ded4d86f3423b2179b8e20059e8f225d5e556e1ce0501f0960"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
